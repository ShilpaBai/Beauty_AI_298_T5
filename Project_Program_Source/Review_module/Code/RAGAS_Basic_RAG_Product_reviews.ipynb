{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HXXl7T6j9Al-",
    "outputId": "e11193d1-08f0-4496-d275-16beefd1820a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide your OpenAI API Key··········\n"
     ]
    }
   ],
   "source": [
    "## Load the Mistral API Key\n",
    "import getpass\n",
    "import os\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Provide your OpenAI API Key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5vdEKI_-9LjR",
    "outputId": "4e79dac6-682c-437c-e2f8-b9171cc5e013"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft was founded by Bill Gates and Paul Allen. They founded the company on April 4, 1975, and their mission was to create software for personal computers. Gates and Allen were bothchildhood friends and had a shared passion for technology and entrepreneurship. They were instrumental in developing the company's early products, including the Microsoft Altair BASIC and the Microsoft Windows operating system. Today, Microsoft is one of the largest and most successful technology companies in the world, with a portfolio of products and services that includes Windows, Office, Azure, and Xbox.]\n"
     ]
    }
   ],
   "source": [
    "# This example is the new way to use the OpenAI lib for python\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "api_key = \"api-key\",\n",
    "base_url = \"https://api.llama-api.com\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-13b-chat\",\n",
    "messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who were the founders of Microsoft?\"}\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "#print(response)\n",
    "#print(response.model_dump_json(indent=2))\n",
    "print(response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pPPHfA689qpG"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EPEACGOD98Mc",
    "outputId": "e28bef9b-5e7f-4325-ef09-7d3d3c5126fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U pip -q\n",
    "!pip install langchain_community langchain_core  langchain faiss-cpu -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GxcndlWt-D3G",
    "outputId": "86ff0800-767e-46e5-8d3b-4b39f195940f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.16.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AyszSzi--fxm",
    "outputId": "04c3e036-d488-4fef-be5b-99e0d84ce600"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.14)\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.16.2)\n",
      "Collecting weaviate-client\n",
      "  Downloading weaviate_client-4.5.5-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting ragas\n",
      "  Downloading ragas-0.1.6-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.30 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.31)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.40)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.40)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
      "Collecting validators==0.22.0 (from weaviate-client)\n",
      "  Downloading validators-0.22.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting authlib<2.0.0,>=1.2.1 (from weaviate-client)\n",
      "  Downloading Authlib-1.3.0-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.57.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.62.1)\n",
      "Collecting grpcio-tools<2.0.0,>=1.57.0 (from weaviate-client)\n",
      "  Downloading grpcio_tools-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Collecting grpcio-health-checking<2.0.0,>=1.57.0 (from weaviate-client)\n",
      "  Downloading grpcio_health_checking-1.62.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (3.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Collecting datasets (from ragas)\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting tiktoken (from ragas)\n",
      "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting langchain-openai (from ragas)\n",
      "  Downloading langchain_openai-0.1.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting pysbd>=0.3.4 (from ragas)\n",
      "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ragas) (1.6.0)\n",
      "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (from authlib<2.0.0,>=1.2.1->weaviate-client) (42.0.5)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Collecting protobuf>=4.21.6 (from grpcio-health-checking<2.0.0,>=1.57.0->weaviate-client)\n",
      "  Downloading protobuf-5.26.1-cp37-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools<2.0.0,>=1.57.0->weaviate-client) (67.7.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.37->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (3.13.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (0.6)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->ragas)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (2.0.3)\n",
      "Collecting xxhash (from datasets->ragas)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->ragas)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets->ragas) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (0.20.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->ragas) (2023.12.25)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas) (2024.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.16.0)\n",
      "Downloading weaviate_client-4.5.5-py3-none-any.whl (306 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.8/306.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
      "Downloading ragas-0.1.6-py3-none-any.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.7/80.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Authlib-1.3.0-py2.py3-none-any.whl (223 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.7/223.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_health_checking-1.62.1-py3-none-any.whl (18 kB)\n",
      "Downloading grpcio_tools-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-0.1.1-py3-none-any.whl (32 kB)\n",
      "Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, validators, pysbd, protobuf, dill, tiktoken, multiprocess, grpcio-tools, grpcio-health-checking, authlib, weaviate-client, datasets, langchain-openai, ragas\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed authlib-1.3.0 datasets-2.18.0 dill-0.3.8 grpcio-health-checking-1.62.1 grpcio-tools-1.62.1 langchain-openai-0.1.1 multiprocess-0.70.16 protobuf-4.25.3 pysbd-0.3.4 ragas-0.1.6 tiktoken-0.6.0 validators-0.22.0 weaviate-client-4.5.5 xxhash-3.4.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langchain openai weaviate-client ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "T9k1QkKDDwI1"
   },
   "outputs": [],
   "source": [
    "OPENAI_API_KEY=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BraUEhLeEZH1"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\n",
    "\n",
    "# Load the data\n",
    "loader = TextLoader('/content/cica.rtf')\n",
    "documents = loader.load()\n",
    "\n",
    "# Chunk the data\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zy7ODwlc9yzb",
    "outputId": "e96dd32b-2441-416b-a146-35f834c6eab7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.10.0)\n",
      "Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/290.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/290.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-4.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_Dc5CLSR9mZJ"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"/content/Product_Reviews.pdf\")\n",
    "docs = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "LpKGAXtk-jcD"
   },
   "outputs": [],
   "source": [
    "# Split text into chunks\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o2YGp2nLEnYt",
    "outputId": "185def16-882c-4599-b807-fe5b89d417cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded weaviate is already listening on port 8079\n",
      "Embedded weaviate wasn't listening on ports http:8079 & grpc:50060, so starting embedded weaviate again\n",
      "Embedded weaviate wasn't listening on ports http:8079 & grpc:50060, so starting embedded weaviate again\n",
      "Started /root/.cache/weaviate-embedded: process ID 7309\n",
      "Started /root/.cache/weaviate-embedded: process ID 7310\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "# Load OpenAI API key from .env file\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Setup vector database\n",
    "client = weaviate.Client(\n",
    "  embedded_options = EmbeddedOptions()\n",
    ")\n",
    "\n",
    "# Populate vector database\n",
    "vectorstore = Weaviate.from_documents(\n",
    "    client = client,\n",
    "    documents = chunks,\n",
    "    embedding = OpenAIEmbeddings(),\n",
    "    by_text = False\n",
    ")\n",
    "\n",
    "# Define vectorstore as retriever to enable semantic search\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kaQu6NiLEqtY",
    "outputId": "189addcf-1511-46ef-e458-720d33976538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "aN51rQDuE5c6"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Define LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Define prompt template\n",
    "template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use two sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Setup RAG pipeline\n",
    "rag_chain = (\n",
    "    {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "10OrpuXP_vwF",
    "outputId": "c1ad9e9e-98c0-4357-cf78-0f29349a940e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"combined_df\",\n  \"rows\": 68,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"What is the primary benefit of the product\\u00a0\",\n          \"What is a user's opinion on the effectiveness of\\u00a0\",\n          \"How does the product generally make the lips\\u00a0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truths\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 52,\n        \"samples\": [\n          \"First night I used this I thought it was\\u00a0\",\n          \"Its not that this is bad but its also not good\\u00a0\",\n          \"Grapefruit is the best in the reviewer's opinion.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "combined_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-ed6b082d-1ea8-46b8-91e6-b20f05a9c249\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the primary use of the product</td>\n",
       "      <td>To ensure softlips by the time lipstick is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product is considered bearable by the</td>\n",
       "      <td>The peppermint scent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does the reviewer describe the product's</td>\n",
       "      <td>Very hydrating and a little goes a long way.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the reviewer's opinion on the product</td>\n",
       "      <td>The reviewer admits the product makes their lips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What negative effect did one reviewer</td>\n",
       "      <td>It would leave a nasty film on their lips after</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>What is the reviewer's favorite scent for the</td>\n",
       "      <td>Grapefruit is the best in the reviewer's opinion.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>What did the reviewer learn after a few weeks</td>\n",
       "      <td>After a few weeks of use, the reviewer learned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Why did the reviewer give the product 2 stars?</td>\n",
       "      <td>The reviewer gave it 2 stars for nice packaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>What was the reviewer's experience with the</td>\n",
       "      <td>The reviewer did not have to reapply it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>What prompted the reviewer to purchase</td>\n",
       "      <td>After running through the sample size from a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 2 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed6b082d-1ea8-46b8-91e6-b20f05a9c249')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-ed6b082d-1ea8-46b8-91e6-b20f05a9c249 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-ed6b082d-1ea8-46b8-91e6-b20f05a9c249');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-1ef515c9-6a77-4f82-9c3c-01dd3e937e7e\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1ef515c9-6a77-4f82-9c3c-01dd3e937e7e')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-1ef515c9-6a77-4f82-9c3c-01dd3e937e7e button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                          question  \\\n",
       "0          What is the primary use of the product    \n",
       "1           product is considered bearable by the    \n",
       "2    How does the reviewer describe the product's    \n",
       "3   What is the reviewer's opinion on the product    \n",
       "4           What negative effect did one reviewer    \n",
       "..                                             ...   \n",
       "63  What is the reviewer's favorite scent for the    \n",
       "64  What did the reviewer learn after a few weeks    \n",
       "65  Why did the reviewer give the product 2 stars?   \n",
       "66    What was the reviewer's experience with the    \n",
       "67         What prompted the reviewer to purchase    \n",
       "\n",
       "                                        ground_truths  \n",
       "0         To ensure softlips by the time lipstick is   \n",
       "1                              The peppermint scent.   \n",
       "2       Very hydrating and a little goes a long way.   \n",
       "3   The reviewer admits the product makes their lips   \n",
       "4    It would leave a nasty film on their lips after   \n",
       "..                                                ...  \n",
       "63  Grapefruit is the best in the reviewer's opinion.  \n",
       "64    After a few weeks of use, the reviewer learned   \n",
       "65    The reviewer gave it 2 stars for nice packaging  \n",
       "66           The reviewer did not have to reapply it   \n",
       "67      After running through the sample size from a   \n",
       "\n",
       "[68 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Import the QA ground truth samples\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Get the path of the folder containing the CSV files\n",
    "folder_path = \"/content/qa\"\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Check if the file is a CSV file\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Read the current CSV file\n",
    "        current_df = pd.read_csv(os.path.join(folder_path, filename))\n",
    "\n",
    "        # Concatenate the current DataFrame with the combined DataFrame\n",
    "        if combined_df.empty:\n",
    "            # If this is the first file, keep the header row\n",
    "            combined_df = current_df\n",
    "        else:\n",
    "            # Otherwise, append the data without the header row\n",
    "            combined_df = pd.concat([combined_df, current_df.iloc[1:]], ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(combined_df.shape)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Bk7SmxRfAPDL"
   },
   "outputs": [],
   "source": [
    "## Use part of ground truth QnAs for demo\n",
    "from datasets import Dataset\n",
    "\n",
    "questions = list(combined_df.question)[0:10]\n",
    "ground_truths = list(combined_df.ground_truths)[0:10]\n",
    "\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "# Inference\n",
    "for query in questions:\n",
    "  answers.append(rag_chain.invoke(query))\n",
    "  contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])\n",
    "\n",
    "# To dict\n",
    "data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truth\": ground_truths\n",
    "}\n",
    "\n",
    "# Convert dict to dataset\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "J5XnR0R3APG4",
    "outputId": "1642ccb1-8c16-4ee1-fec3-11597e133023"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"What was the main reason for purchasing\\u00a0\",\n          \"product is considered bearable by the\\u00a0\",\n          \"What is the reviewer's opinion on the\\u00a0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"A downside of the product mentioned is that it can lead to blackheads, clogged pores, and pimples around the lips with continued use.\",\n          \"The product is considered bearable by some users, but others find it overpriced and not worth the hype due to lack of hydration and other issues.\",\n          \"The reviewer's opinion on the lip mask is mixed, with some positive comments about hydration and scent, but also negative feedback regarding dryness and skin issues.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truth\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"It's really sticky and stains pillow cases.\\u00a0\",\n          \"The peppermint scent.\\u00a0\",\n          \"The reviewer does not think it is overpriced\\u00a0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-5133e78b-721f-42f3-ab38-0df13bb28dcd\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the primary use of the product</td>\n",
       "      <td>The primary use of the product is to hydrate a...</td>\n",
       "      <td>[This product is amazing It really does smell ...</td>\n",
       "      <td>To ensure softlips by the time lipstick is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product is considered bearable by the</td>\n",
       "      <td>The product is considered bearable by some use...</td>\n",
       "      <td>[This product is amazing It really does smell ...</td>\n",
       "      <td>The peppermint scent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does the reviewer describe the product's</td>\n",
       "      <td>The reviewer describes the product as having a...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>Very hydrating and a little goes a long way.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the reviewer's opinion on the product</td>\n",
       "      <td>The reviewer's opinion on the product is mixed...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>The reviewer admits the product makes their lips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What negative effect did one reviewer</td>\n",
       "      <td>One reviewer experienced more blackheads, clog...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>It would leave a nasty film on their lips after</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is the reviewer's opinion on the</td>\n",
       "      <td>The reviewer's opinion on the lip mask is mixe...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>The reviewer does not think it is overpriced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does the product smell according to one</td>\n",
       "      <td>The product smells like gummy bears according ...</td>\n",
       "      <td>[This product is amazing It really does smell ...</td>\n",
       "      <td>It smells really good.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What was the main reason for purchasing</td>\n",
       "      <td>The main reason for purchasing the lip mask wa...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>The reviewer bought it mainly for the scent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is a downside of the product mentioned</td>\n",
       "      <td>A downside of the product mentioned is that it...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>It's really sticky and stains pillow cases.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is the reviewer's opinion on the</td>\n",
       "      <td>The reviewer's opinion on the lip balm is mixe...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>It works but not for longer than a few hours</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5133e78b-721f-42f3-ab38-0df13bb28dcd')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-5133e78b-721f-42f3-ab38-0df13bb28dcd button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-5133e78b-721f-42f3-ab38-0df13bb28dcd');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-1d9114d4-5a2d-4ab5-aa66-170c3b0f0030\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1d9114d4-5a2d-4ab5-aa66-170c3b0f0030')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-1d9114d4-5a2d-4ab5-aa66-170c3b0f0030 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                         question  \\\n",
       "0         What is the primary use of the product    \n",
       "1          product is considered bearable by the    \n",
       "2   How does the reviewer describe the product's    \n",
       "3  What is the reviewer's opinion on the product    \n",
       "4          What negative effect did one reviewer    \n",
       "5          What is the reviewer's opinion on the    \n",
       "6    How does the product smell according to one    \n",
       "7        What was the main reason for purchasing    \n",
       "8    What is a downside of the product mentioned    \n",
       "9          What is the reviewer's opinion on the    \n",
       "\n",
       "                                              answer  \\\n",
       "0  The primary use of the product is to hydrate a...   \n",
       "1  The product is considered bearable by some use...   \n",
       "2  The reviewer describes the product as having a...   \n",
       "3  The reviewer's opinion on the product is mixed...   \n",
       "4  One reviewer experienced more blackheads, clog...   \n",
       "5  The reviewer's opinion on the lip mask is mixe...   \n",
       "6  The product smells like gummy bears according ...   \n",
       "7  The main reason for purchasing the lip mask wa...   \n",
       "8  A downside of the product mentioned is that it...   \n",
       "9  The reviewer's opinion on the lip balm is mixe...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [This product is amazing It really does smell ...   \n",
       "1  [This product is amazing It really does smell ...   \n",
       "2  [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "3  [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "4  [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "5  [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "6  [This product is amazing It really does smell ...   \n",
       "7  [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "8  [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "9  [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "\n",
       "                                        ground_truth  \n",
       "0        To ensure softlips by the time lipstick is   \n",
       "1                             The peppermint scent.   \n",
       "2      Very hydrating and a little goes a long way.   \n",
       "3  The reviewer admits the product makes their lips   \n",
       "4   It would leave a nasty film on their lips after   \n",
       "5      The reviewer does not think it is overpriced   \n",
       "6                            It smells really good.   \n",
       "7      The reviewer bought it mainly for the scent.   \n",
       "8       It's really sticky and stains pillow cases.   \n",
       "9      It works but not for longer than a few hours   "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569,
     "referenced_widgets": [
      "83010bccbd8c4f0c916a318d3f211a96",
      "31d9968f13dd40759a3276b44649ae3d",
      "ac4ddaeb47b3456d87529f80ce4ae266",
      "6ef40e9c23884139a7a8dbd2d96d602c",
      "08cc0a022efd4abe9ced356efe614e19",
      "747cb5eefa8d4dfdbce3f7375cb5e157",
      "d72fec8d51544ed8a03a26778e444832",
      "0ac8fd9a81624e74afbff7e3f5c8b3c8",
      "980f4bd382d14f8b9c62ac54f3a16068",
      "13bcc8d456df484d864c568114478bd9",
      "27aa99cab69a4814b806756947fe1656"
     ]
    },
    "id": "OkcCXqNiAPJ9",
    "outputId": "54caa403-caa2-4b92-efab-147b7e54461c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83010bccbd8c4f0c916a318d3f211a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"result\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"What was the main reason for purchasing\\u00a0\",\n          \"product is considered bearable by the\\u00a0\",\n          \"What is the reviewer's opinion on the\\u00a0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"A downside of the product mentioned is that it can lead to blackheads, clogged pores, and pimples around the lips with continued use.\",\n          \"The product is considered bearable by some users, but others find it overpriced and not worth the hype due to lack of hydration and other issues.\",\n          \"The reviewer's opinion on the lip mask is mixed, with some positive comments about hydration and scent, but also negative feedback regarding dryness and skin issues.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truth\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"It's really sticky and stains pillow cases.\\u00a0\",\n          \"The peppermint scent.\\u00a0\",\n          \"The reviewer does not think it is overpriced\\u00a0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_relevancy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06417715696474685,\n        \"min\": 0.8043880970371036,\n        \"max\": 0.9806268659024702,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.9805806172632651,\n          0.8043880970371036,\n          0.844462701544955\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04448694669759544,\n        \"min\": 0.7529295603147164,\n        \"max\": 0.901476955242232,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.806870377410349,\n          0.7529295603147164,\n          0.7972807322782831\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_correctness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2346852110713573,\n        \"min\": 0.1882323900786791,\n        \"max\": 0.7117292814223966,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.20171759435258724,\n          0.1882323900786791,\n          0.19932018306957078\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.32080360343650144,\n        \"min\": 0.0,\n        \"max\": 0.999999999975,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.6388888888675925,\n          0.0,\n          0.9999999999666667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4216370213557839,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"faithfulness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.31622776601683794,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-cf90da78-a797-4b60-b415-00f29bf5a9dc\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>answer_similarity</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the primary use of the product</td>\n",
       "      <td>The primary use of the product is to hydrate a...</td>\n",
       "      <td>[This product is amazing It really does smell ...</td>\n",
       "      <td>To ensure softlips by the time lipstick is</td>\n",
       "      <td>0.975340</td>\n",
       "      <td>0.854045</td>\n",
       "      <td>0.213511</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product is considered bearable by the</td>\n",
       "      <td>The product is considered bearable by some use...</td>\n",
       "      <td>[This product is amazing It really does smell ...</td>\n",
       "      <td>The peppermint scent.</td>\n",
       "      <td>0.804388</td>\n",
       "      <td>0.752930</td>\n",
       "      <td>0.188232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does the reviewer describe the product's</td>\n",
       "      <td>The reviewer describes the product as having a...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>Very hydrating and a little goes a long way.</td>\n",
       "      <td>0.948752</td>\n",
       "      <td>0.846917</td>\n",
       "      <td>0.711729</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the reviewer's opinion on the product</td>\n",
       "      <td>The reviewer's opinion on the product is mixed...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>The reviewer admits the product makes their lips</td>\n",
       "      <td>0.980627</td>\n",
       "      <td>0.901477</td>\n",
       "      <td>0.600369</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What negative effect did one reviewer</td>\n",
       "      <td>One reviewer experienced more blackheads, clog...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>It would leave a nasty film on their lips after</td>\n",
       "      <td>0.862161</td>\n",
       "      <td>0.811731</td>\n",
       "      <td>0.202933</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is the reviewer's opinion on the</td>\n",
       "      <td>The reviewer's opinion on the lip mask is mixe...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>The reviewer does not think it is overpriced</td>\n",
       "      <td>0.844463</td>\n",
       "      <td>0.797281</td>\n",
       "      <td>0.199320</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does the product smell according to one</td>\n",
       "      <td>The product smells like gummy bears according ...</td>\n",
       "      <td>[This product is amazing It really does smell ...</td>\n",
       "      <td>It smells really good.</td>\n",
       "      <td>0.929578</td>\n",
       "      <td>0.845629</td>\n",
       "      <td>0.711407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What was the main reason for purchasing</td>\n",
       "      <td>The main reason for purchasing the lip mask wa...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>The reviewer bought it mainly for the scent.</td>\n",
       "      <td>0.868165</td>\n",
       "      <td>0.800987</td>\n",
       "      <td>0.200247</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is a downside of the product mentioned</td>\n",
       "      <td>A downside of the product mentioned is that it...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>It's really sticky and stains pillow cases.</td>\n",
       "      <td>0.980581</td>\n",
       "      <td>0.806870</td>\n",
       "      <td>0.201718</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is the reviewer's opinion on the</td>\n",
       "      <td>The reviewer's opinion on the lip balm is mixe...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>It works but not for longer than a few hours</td>\n",
       "      <td>0.870198</td>\n",
       "      <td>0.764810</td>\n",
       "      <td>0.566202</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf90da78-a797-4b60-b415-00f29bf5a9dc')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-cf90da78-a797-4b60-b415-00f29bf5a9dc button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-cf90da78-a797-4b60-b415-00f29bf5a9dc');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-4e53d50e-82b8-4c8d-9146-0e54694b915e\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4e53d50e-82b8-4c8d-9146-0e54694b915e')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-4e53d50e-82b8-4c8d-9146-0e54694b915e button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                         question  \\\n",
       "0         What is the primary use of the product    \n",
       "1          product is considered bearable by the    \n",
       "2   How does the reviewer describe the product's    \n",
       "3  What is the reviewer's opinion on the product    \n",
       "4          What negative effect did one reviewer    \n",
       "5          What is the reviewer's opinion on the    \n",
       "6    How does the product smell according to one    \n",
       "7        What was the main reason for purchasing    \n",
       "8    What is a downside of the product mentioned    \n",
       "9          What is the reviewer's opinion on the    \n",
       "\n",
       "                                              answer  \\\n",
       "0  The primary use of the product is to hydrate a...   \n",
       "1  The product is considered bearable by some use...   \n",
       "2  The reviewer describes the product as having a...   \n",
       "3  The reviewer's opinion on the product is mixed...   \n",
       "4  One reviewer experienced more blackheads, clog...   \n",
       "5  The reviewer's opinion on the lip mask is mixe...   \n",
       "6  The product smells like gummy bears according ...   \n",
       "7  The main reason for purchasing the lip mask wa...   \n",
       "8  A downside of the product mentioned is that it...   \n",
       "9  The reviewer's opinion on the lip balm is mixe...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [This product is amazing It really does smell ...   \n",
       "1  [This product is amazing It really does smell ...   \n",
       "2  [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "3  [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "4  [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "5  [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "6  [This product is amazing It really does smell ...   \n",
       "7  [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "8  [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "9  [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "\n",
       "                                        ground_truth  answer_relevancy  \\\n",
       "0        To ensure softlips by the time lipstick is           0.975340   \n",
       "1                             The peppermint scent.           0.804388   \n",
       "2      Very hydrating and a little goes a long way.           0.948752   \n",
       "3  The reviewer admits the product makes their lips           0.980627   \n",
       "4   It would leave a nasty film on their lips after           0.862161   \n",
       "5      The reviewer does not think it is overpriced           0.844463   \n",
       "6                            It smells really good.           0.929578   \n",
       "7      The reviewer bought it mainly for the scent.           0.868165   \n",
       "8       It's really sticky and stains pillow cases.           0.980581   \n",
       "9      It works but not for longer than a few hours           0.870198   \n",
       "\n",
       "   answer_similarity  answer_correctness  context_precision  context_recall  \\\n",
       "0           0.854045            0.213511           0.638889             1.0   \n",
       "1           0.752930            0.188232           0.000000             1.0   \n",
       "2           0.846917            0.711729           1.000000             1.0   \n",
       "3           0.901477            0.600369           1.000000             1.0   \n",
       "4           0.811731            0.202933           1.000000             1.0   \n",
       "5           0.797281            0.199320           1.000000             1.0   \n",
       "6           0.845629            0.711407           1.000000             1.0   \n",
       "7           0.800987            0.200247           1.000000             1.0   \n",
       "8           0.806870            0.201718           1.000000             0.0   \n",
       "9           0.764810            0.566202           0.805556             0.0   \n",
       "\n",
       "   faithfulness  \n",
       "0           1.0  \n",
       "1           1.0  \n",
       "2           1.0  \n",
       "3           1.0  \n",
       "4           1.0  \n",
       "5           1.0  \n",
       "6           1.0  \n",
       "7           1.0  \n",
       "8           0.0  \n",
       "9           1.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    answer_similarity,\n",
    "    answer_correctness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    faithfulness,\n",
    "\n",
    ")\n",
    "\n",
    "result = evaluate(\n",
    "    dataset = dataset,\n",
    "    metrics=[\n",
    "        answer_relevancy,    # Scores the relevancy of the answer according to the given question.\n",
    "        answer_similarity,  # Scores the semantic similarity of ground truth with generated answer.\n",
    "        answer_correctness, # Measures answer correctness compared to ground truth as a combination of factuality and semantic similarity.\n",
    "        context_precision,  # Average Precision is a metric that evaluates whether all of the relevant items selected by the model are ranked higher or not.\n",
    "        context_recall,     # Estimates context recall by estimating TP and FN using annotated answer and retrieved context.\n",
    "        faithfulness,        # measures the factual consistency of the generated answer against the given context.\n",
    "\n",
    "\n",
    "    ],\n",
    ")\n",
    "\n",
    "result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "iz3F97uYBoaz"
   },
   "outputs": [],
   "source": [
    "df = result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "cJ5si5KvBody",
    "outputId": "75cf6168-8e37-4733-efea-e54519371998"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"answer_relevancy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.2720514290817766,\n        \"min\": 0.06417715696474685,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.9064252485979434,\n          0.8998879990122641,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.295940928768545,\n        \"min\": 0.04448694669759544,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8182676857587567,\n          0.8093007047622647,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_correctness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.414197871159457,\n        \"min\": 0.1882323900786791,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.3795669214396892,\n          0.20822203293881808,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.302547712873212,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8444444444037963,\n          0.999999999925,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.2918839281858725,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.8,\n          1.0,\n          0.4216370213557839\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"faithfulness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.294330661944358,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.9,\n          1.0,\n          0.31622776601683794\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-266b11e1-9b33-4bb0-b1ee-23fd20ebfa0a\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>answer_similarity</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.906425</td>\n",
       "      <td>0.818268</td>\n",
       "      <td>0.379567</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.064177</td>\n",
       "      <td>0.044487</td>\n",
       "      <td>0.234685</td>\n",
       "      <td>0.320804</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>0.316228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.804388</td>\n",
       "      <td>0.752930</td>\n",
       "      <td>0.188232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.863662</td>\n",
       "      <td>0.798207</td>\n",
       "      <td>0.200614</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.899888</td>\n",
       "      <td>0.809301</td>\n",
       "      <td>0.208222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.968693</td>\n",
       "      <td>0.846595</td>\n",
       "      <td>0.591828</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.980627</td>\n",
       "      <td>0.901477</td>\n",
       "      <td>0.711729</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-266b11e1-9b33-4bb0-b1ee-23fd20ebfa0a')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-266b11e1-9b33-4bb0-b1ee-23fd20ebfa0a button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-266b11e1-9b33-4bb0-b1ee-23fd20ebfa0a');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-06a43b6b-aa90-44f5-9086-7eb372ea2fe6\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-06a43b6b-aa90-44f5-9086-7eb372ea2fe6')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-06a43b6b-aa90-44f5-9086-7eb372ea2fe6 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "       answer_relevancy  answer_similarity  answer_correctness  \\\n",
       "count         10.000000          10.000000           10.000000   \n",
       "mean           0.906425           0.818268            0.379567   \n",
       "std            0.064177           0.044487            0.234685   \n",
       "min            0.804388           0.752930            0.188232   \n",
       "25%            0.863662           0.798207            0.200614   \n",
       "50%            0.899888           0.809301            0.208222   \n",
       "75%            0.968693           0.846595            0.591828   \n",
       "max            0.980627           0.901477            0.711729   \n",
       "\n",
       "       context_precision  context_recall  faithfulness  \n",
       "count          10.000000       10.000000     10.000000  \n",
       "mean            0.844444        0.800000      0.900000  \n",
       "std             0.320804        0.421637      0.316228  \n",
       "min             0.000000        0.000000      0.000000  \n",
       "25%             0.854167        1.000000      1.000000  \n",
       "50%             1.000000        1.000000      1.000000  \n",
       "75%             1.000000        1.000000      1.000000  \n",
       "max             1.000000        1.000000      1.000000  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGeuOl-0BohG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZLP1WBd8CCQ"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Define LLM\n",
    "llm = ChatMistralAI(model=\"open-mixtral-8x7b\") #mistral_api_key=api_key\n",
    "#llm = ChatOpenAI()\n",
    "\n",
    "# Define prompt template\n",
    "template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use two sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Setup RAG pipeline\n",
    "rag_chain = (\n",
    "    {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3pfoJh1F84L"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "questions=['Did the Cica Sleeping Mask hydrate the face?',\n",
    " 'Did the Cica Sleeping Mask cause any breakouts?',\n",
    " 'How many times did the user apply the Cica Sleeping Mask before being impressed?',\n",
    " \"Is the Cica Sleeping Mask described as a 'must buy'?\",\n",
    " \"Is the product considered a 'holy grail' for the user?\",\n",
    " \"Did the user see a difference in their skin after one night's use?\",\n",
    " 'Is the Cica Sleeping Mask better than the water mask according to the user?',\n",
    " 'Did the product help with irritation or dry spots overnight?',\n",
    " 'Has the user been using the Cica Sleeping Mask for a long time?',\n",
    " 'Is the Cica Sleeping Mask good for dry and acne-prone skin?']\n",
    "\n",
    "\n",
    "ground_truths=[['Yes'],\n",
    " ['Yes'],\n",
    " ['4 times'],\n",
    " ['Yes'],\n",
    " ['Yes, it is super hydrating and perfect for daytime wear under makeup and nighttime use.'],\n",
    " ['Yes'],\n",
    " ['Yes'],\n",
    " ['Yes'],\n",
    " ['Yes, for more than 3 years.'],\n",
    " ['Yes']]\n",
    "\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "# Inference\n",
    "for query in questions:\n",
    "  answers.append(rag_chain.invoke(query))\n",
    "  contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])\n",
    "\n",
    "# To dict\n",
    "data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truths\": ground_truths\n",
    "}\n",
    "\n",
    "# Convert dict to dataset\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M4dmot3AMk40",
    "outputId": "5f3b79b7-da79-4c32-fb01-5421516f64f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'contexts', 'ground_truths'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "f22036d508534b598b63e3d197d14e56",
      "ad8cc25ac5524edda74bdc207d56446d",
      "f9833040f47a49f1ad8384a1182923f3",
      "71ab2aa9af3f46d49f9eaa9f34e4ad7e",
      "563b4ec88b1344619816369cd217b8df",
      "8a322a3b98334ef99446e301c387aafc",
      "3a7aa07d529e4b7dad80002b7ff814f2",
      "91759ccd9a364ac5ae8206abf1870437",
      "ca4d63c53a8d448e9a2f0ffde17afc08",
      "bb960b4ac0ef42ddac65f62fb656e7e7",
      "11233b1143f940c7a512e1da1b72dffe"
     ]
    },
    "id": "Qmwy3KUwGUtD",
    "outputId": "14e87bc5-71d0-469e-a574-fb5962b6aab3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:ragas.validation:passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22036d508534b598b63e3d197d14e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:ragas.metrics._answer_relevance:Invalid JSON response. Expected dictionary with key 'question'\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "result = evaluate(\n",
    "    dataset = dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "    ],\n",
    ")\n",
    "\n",
    "df = result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vyDnT0kxGfPK",
    "outputId": "5e7848d5-480e-4198-d17a-744fdc4d4593"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Has the user been using the Cica Sleeping Mask for a long time?\",\n          \"Did the Cica Sleeping Mask cause any breakouts?\",\n          \"Did the user see a difference in their skin after one night's use?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Yes, the user has been using the Cica Sleeping Mask for 2 years.\",\n          \"No, the Cica Sleeping Mask did not cause any breakouts according to the reviews and experiences shared.\",\n          \"Yes, the user saw a difference in their skin after one night's use of the Cica Sleeping mask.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truths\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truth\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"4 times\",\n          \"Yes, for more than 3 years.\",\n          \"Yes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4830458914947937,\n        \"min\": 0.0,\n        \"max\": 0.99999999995,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.9999999999,\n          0.99999999995,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.31622776601683794,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"faithfulness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.466645407679039,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0,\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_relevancy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06343785594509502,\n        \"min\": 0.803104269041423,\n        \"max\": 0.9999999999999998,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.8976275782584641,\n          0.981866334305785\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-cc42aa6c-b488-4c91-9deb-fea3e6624380\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truths</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Did the Cica Sleeping Mask hydrate the face?</td>\n",
       "      <td>Yes, the Cica Sleeping Mask hydrated the face ...</td>\n",
       "      <td>[\\f0\\fs20 \\cf2 Cica Sleeping mask\\\\nDid it hyd...</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Did the Cica Sleeping Mask cause any breakouts?</td>\n",
       "      <td>No, the Cica Sleeping Mask did not cause any b...</td>\n",
       "      <td>[\\f0\\fs20 \\cf2 Cica Sleeping mask\\\\nDid it hyd...</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.981866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How many times did the user apply the Cica Sle...</td>\n",
       "      <td>The user applied the Cica Sleeping Mask 4 time...</td>\n",
       "      <td>[\\f0\\fs20 \\cf2 Cica Sleeping mask\\\\nDid it hyd...</td>\n",
       "      <td>[4 times]</td>\n",
       "      <td>4 times</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.803104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is the Cica Sleeping Mask described as a 'must...</td>\n",
       "      <td>Yes, the Cica Sleeping Mask is described as a ...</td>\n",
       "      <td>[\\f0\\fs20 \\cf2 Cica Sleeping mask\\\\nDid it hyd...</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.892311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is the product considered a 'holy grail' for t...</td>\n",
       "      <td>Yes, the product is considered a 'holy grail' ...</td>\n",
       "      <td>[\\f0\\fs20 \\cf2 Cica Sleeping mask\\\\nDid it hyd...</td>\n",
       "      <td>[Yes, it is super hydrating and perfect for da...</td>\n",
       "      <td>Yes, it is super hydrating and perfect for day...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Did the user see a difference in their skin af...</td>\n",
       "      <td>Yes, the user saw a difference in their skin a...</td>\n",
       "      <td>[\\f0\\fs20 \\cf2 Cica Sleeping mask\\\\nDid it hyd...</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.825576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Is the Cica Sleeping Mask better than the wate...</td>\n",
       "      <td>Yes, the user mentioned that the Cica Sleeping...</td>\n",
       "      <td>[\\f0\\fs20 \\cf2 Cica Sleeping mask\\\\nDid it hyd...</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.912128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Did the product help with irritation or dry sp...</td>\n",
       "      <td>Yes, the product helped with irritation and dr...</td>\n",
       "      <td>[\\f0\\fs20 \\cf2 Cica Sleeping mask\\\\nDid it hyd...</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Has the user been using the Cica Sleeping Mask...</td>\n",
       "      <td>Yes, the user has been using the Cica Sleeping...</td>\n",
       "      <td>[\\f0\\fs20 \\cf2 Cica Sleeping mask\\\\nDid it hyd...</td>\n",
       "      <td>[Yes, for more than 3 years.]</td>\n",
       "      <td>Yes, for more than 3 years.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.897628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Is the Cica Sleeping Mask good for dry and acn...</td>\n",
       "      <td>Yes, the Cica Sleeping Mask is good for dry an...</td>\n",
       "      <td>[\\f0\\fs20 \\cf2 Cica Sleeping mask\\\\nDid it hyd...</td>\n",
       "      <td>[Yes]</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc42aa6c-b488-4c91-9deb-fea3e6624380')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-cc42aa6c-b488-4c91-9deb-fea3e6624380 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-cc42aa6c-b488-4c91-9deb-fea3e6624380');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-66a8fe99-3651-4dc9-a79f-05b88fde4fe7\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-66a8fe99-3651-4dc9-a79f-05b88fde4fe7')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-66a8fe99-3651-4dc9-a79f-05b88fde4fe7 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0       Did the Cica Sleeping Mask hydrate the face?   \n",
       "1    Did the Cica Sleeping Mask cause any breakouts?   \n",
       "2  How many times did the user apply the Cica Sle...   \n",
       "3  Is the Cica Sleeping Mask described as a 'must...   \n",
       "4  Is the product considered a 'holy grail' for t...   \n",
       "5  Did the user see a difference in their skin af...   \n",
       "6  Is the Cica Sleeping Mask better than the wate...   \n",
       "7  Did the product help with irritation or dry sp...   \n",
       "8  Has the user been using the Cica Sleeping Mask...   \n",
       "9  Is the Cica Sleeping Mask good for dry and acn...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Yes, the Cica Sleeping Mask hydrated the face ...   \n",
       "1  No, the Cica Sleeping Mask did not cause any b...   \n",
       "2  The user applied the Cica Sleeping Mask 4 time...   \n",
       "3  Yes, the Cica Sleeping Mask is described as a ...   \n",
       "4  Yes, the product is considered a 'holy grail' ...   \n",
       "5  Yes, the user saw a difference in their skin a...   \n",
       "6  Yes, the user mentioned that the Cica Sleeping...   \n",
       "7  Yes, the product helped with irritation and dr...   \n",
       "8  Yes, the user has been using the Cica Sleeping...   \n",
       "9  Yes, the Cica Sleeping Mask is good for dry an...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [\\f0\\fs20 \\cf2 Cica Sleeping mask\\\\nDid it hyd...   \n",
       "1  [\\f0\\fs20 \\cf2 Cica Sleeping mask\\\\nDid it hyd...   \n",
       "2  [\\f0\\fs20 \\cf2 Cica Sleeping mask\\\\nDid it hyd...   \n",
       "3  [\\f0\\fs20 \\cf2 Cica Sleeping mask\\\\nDid it hyd...   \n",
       "4  [\\f0\\fs20 \\cf2 Cica Sleeping mask\\\\nDid it hyd...   \n",
       "5  [\\f0\\fs20 \\cf2 Cica Sleeping mask\\\\nDid it hyd...   \n",
       "6  [\\f0\\fs20 \\cf2 Cica Sleeping mask\\\\nDid it hyd...   \n",
       "7  [\\f0\\fs20 \\cf2 Cica Sleeping mask\\\\nDid it hyd...   \n",
       "8  [\\f0\\fs20 \\cf2 Cica Sleeping mask\\\\nDid it hyd...   \n",
       "9  [\\f0\\fs20 \\cf2 Cica Sleeping mask\\\\nDid it hyd...   \n",
       "\n",
       "                                       ground_truths  \\\n",
       "0                                              [Yes]   \n",
       "1                                              [Yes]   \n",
       "2                                          [4 times]   \n",
       "3                                              [Yes]   \n",
       "4  [Yes, it is super hydrating and perfect for da...   \n",
       "5                                              [Yes]   \n",
       "6                                              [Yes]   \n",
       "7                                              [Yes]   \n",
       "8                      [Yes, for more than 3 years.]   \n",
       "9                                              [Yes]   \n",
       "\n",
       "                                        ground_truth  context_precision  \\\n",
       "0                                                Yes                1.0   \n",
       "1                                                Yes                1.0   \n",
       "2                                            4 times                1.0   \n",
       "3                                                Yes                1.0   \n",
       "4  Yes, it is super hydrating and perfect for day...                1.0   \n",
       "5                                                Yes                1.0   \n",
       "6                                                Yes                0.0   \n",
       "7                                                Yes                1.0   \n",
       "8                        Yes, for more than 3 years.                0.0   \n",
       "9                                                Yes                0.0   \n",
       "\n",
       "   context_recall  faithfulness  answer_relevancy  \n",
       "0             0.0      1.000000          0.910587  \n",
       "1             1.0           NaN          0.981866  \n",
       "2             1.0      0.000000          0.803104  \n",
       "3             1.0      0.333333          0.892311  \n",
       "4             1.0      0.500000               NaN  \n",
       "5             1.0      1.000000          0.825576  \n",
       "6             1.0           NaN          0.912128  \n",
       "7             1.0      1.000000          0.884270  \n",
       "8             1.0      0.000000          0.897628  \n",
       "9             1.0      0.000000          1.000000  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-qvQS-MHhtE"
   },
   "outputs": [],
   "source": [
    "qa_pairs = {\n",
    "        \"Did the Cica Sleeping Mask hydrate the face?\": \"Yes\",\n",
    "        \"Did the Cica Sleeping Mask cause any breakouts?\": \"Yes\",\n",
    "        \"How many times did the user apply the Cica Sleeping Mask before being impressed?\": \"4 times\",\n",
    "        \"Is the Cica Sleeping Mask described as a 'must buy'?\": \"Yes\",\n",
    "        \"Is the product considered a 'holy grail' for the user?\": \"Yes, it is super hydrating and perfect for daytime wear under makeup and nighttime use.\",\n",
    "        \"Did the user see a difference in their skin after one night's use?\": \"Yes\",\n",
    "        \"Is the Cica Sleeping Mask better than the water mask according to the user?\": \"Yes\",\n",
    "        \"Did the product help with irritation or dry spots overnight?\": \"Yes\",\n",
    "        \"Has the user been using the Cica Sleeping Mask for a long time?\": \"Yes, for more than 3 years.\",\n",
    "        \"Is the Cica Sleeping Mask good for dry and acne-prone skin?\": \"Yes\"\n",
    "    }\n",
    "\n",
    "    # Create a list of questions and a list of ground truth answers\n",
    "questions = []\n",
    "ground_truths = []\n",
    "for question, answer in qa_pairs.items():\n",
    "    questions.append(question)\n",
    "    ground_truths.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UrQIC9vcLPbj"
   },
   "outputs": [],
   "source": [
    "questions=['Did the Cica Sleeping Mask hydrate the face?',\n",
    " 'Did the Cica Sleeping Mask cause any breakouts?',\n",
    " 'How many times did the user apply the Cica Sleeping Mask before being impressed?',\n",
    " \"Is the Cica Sleeping Mask described as a 'must buy'?\",\n",
    " \"Is the product considered a 'holy grail' for the user?\",\n",
    " \"Did the user see a difference in their skin after one night's use?\",\n",
    " 'Is the Cica Sleeping Mask better than the water mask according to the user?',\n",
    " 'Did the product help with irritation or dry spots overnight?',\n",
    " 'Has the user been using the Cica Sleeping Mask for a long time?',\n",
    " 'Is the Cica Sleeping Mask good for dry and acne-prone skin?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnzZT0nLLQvA"
   },
   "outputs": [],
   "source": [
    "ground_truths=['Yes',\n",
    " 'Yes',\n",
    " '4 times',\n",
    " 'Yes',\n",
    " 'Yes, it is super hydrating and perfect for daytime wear under makeup and nighttime use.',\n",
    " 'Yes',\n",
    " 'Yes',\n",
    " 'Yes',\n",
    " 'Yes, for more than 3 years.',\n",
    " 'Yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "569NLWy-LSnj"
   },
   "outputs": [],
   "source": [
    "g=[[i] for i in ground_truths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mo9Kz5ckM6k6",
    "outputId": "13d812ab-d76d-4bc5-da0d-c9125297758a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Yes'],\n",
       " ['Yes'],\n",
       " ['4 times'],\n",
       " ['Yes'],\n",
       " ['Yes, it is super hydrating and perfect for daytime wear under makeup and nighttime use.'],\n",
       " ['Yes'],\n",
       " ['Yes'],\n",
       " ['Yes'],\n",
       " ['Yes, for more than 3 years.'],\n",
       " ['Yes']]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sMRvwe1jNBre"
   },
   "outputs": [],
   "source": [
    "r=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9x4Kvc0u7L_l"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain_community.chat_models import ChatAnyscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gEw6y-lvyqyJ"
   },
   "outputs": [],
   "source": [
    "llm = ChatAnyscale(model=\"meta-llama/Llama-2-13b-chat-hf\", anyscale_api_key='esecret_5datfql4wjbg1mqgshlvutaary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h7ZfDSFiyq1N",
    "outputId": "55ca5577-96bf-45e9-84b5-ad617f2294bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.16.2-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
      "Downloading openai-1.16.2-py3-none-any.whl (267 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
      "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.16.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "S40xZ1TFv-Rm"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain_community.chat_models import ChatAnyscale\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "llm = ChatAnyscale(model=\"meta-llama/Llama-2-13b-chat-hf\", anyscale_api_key='esecret_5datfql4wjbg1mqgshlvutaary')\n",
    "\n",
    "# Define prompt template\n",
    "template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just try your best to answer it.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Setup RAG pipeline\n",
    "rag_chain = (\n",
    "    {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "rARy5YVL25nm",
    "outputId": "de73331f-883d-4743-f8ad-7f87f2e3e211"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"combined_df\",\n  \"rows\": 69,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"What is the primary benefit of the product\\u00a0\",\n          \"What is a user's opinion on the effectiveness of\\u00a0\",\n          \"How does the product generally make the lips\\u00a0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truths\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 52,\n        \"samples\": [\n          \"First night I used this I thought it was\\u00a0\",\n          \"Its not that this is bad but its also not good\\u00a0\",\n          \"Grapefruit is the best in the reviewer's opinion.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "combined_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-81c80cc4-3eb4-465b-9fc6-a88ed5f1ed0e\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truths</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the primary use of the product</td>\n",
       "      <td>To ensure softlips by the time lipstick is</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product is considered bearable by the</td>\n",
       "      <td>The peppermint scent.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does the reviewer describe the product's</td>\n",
       "      <td>Very hydrating and a little goes a long way.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the reviewer's opinion on the product</td>\n",
       "      <td>The reviewer admits the product makes their lips</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What negative effect did one reviewer</td>\n",
       "      <td>It would leave a nasty film on their lips after</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-81c80cc4-3eb4-465b-9fc6-a88ed5f1ed0e')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-81c80cc4-3eb4-465b-9fc6-a88ed5f1ed0e button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-81c80cc4-3eb4-465b-9fc6-a88ed5f1ed0e');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-7d6feced-b76d-4e03-94fd-2a1033063b2e\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7d6feced-b76d-4e03-94fd-2a1033063b2e')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-7d6feced-b76d-4e03-94fd-2a1033063b2e button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                         question  \\\n",
       "0         What is the primary use of the product    \n",
       "1          product is considered bearable by the    \n",
       "2   How does the reviewer describe the product's    \n",
       "3  What is the reviewer's opinion on the product    \n",
       "4          What negative effect did one reviewer    \n",
       "\n",
       "                                       ground_truths  Unnamed: 2  Unnamed: 3  \n",
       "0        To ensure softlips by the time lipstick is          NaN         NaN  \n",
       "1                             The peppermint scent.          NaN         NaN  \n",
       "2      Very hydrating and a little goes a long way.          NaN         NaN  \n",
       "3  The reviewer admits the product makes their lips          NaN         NaN  \n",
       "4   It would leave a nasty film on their lips after          NaN         NaN  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "PgJkxoHA29vW"
   },
   "outputs": [],
   "source": [
    "## Use part of ground truth QnAs for demo\n",
    "from datasets import Dataset\n",
    "\n",
    "questions = list(combined_df.question)\n",
    "ground_truths = list(combined_df.ground_truths)\n",
    "\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "# Inference\n",
    "for query in questions:\n",
    "  answers.append(rag_chain.invoke(query))\n",
    "  contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query, k=2)])\n",
    "\n",
    "# To dict\n",
    "data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truth\": ground_truths\n",
    "}\n",
    "#5m\n",
    "# Convert dict to dataset\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "J7Pad2EP3DTh",
    "outputId": "be0a6b24-12fc-4460-9189-da6c614cb340"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 68,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"What is the primary benefit of the product\\u00a0\",\n          \"What is a user's opinion on the effectiveness of\\u00a0\",\n          \"How does the product generally make the lips\\u00a0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 68,\n        \"samples\": [\n          \"  Based on the provided context, the specific flavor of the product that the person is looking for is:\\n\\n\\\"Sweet Candy\\\"\\n\\nThe document mentions the following flavors:\\n\\n* Peppermint\\n* Grapefruit\\n* Gummy Bear\\n* Sweet Candy\\n\\nTherefore, the answer to the question \\\"What specific flavor of the product is...\\\" would be:\\n\\n\\\"Sweet Candy\\\"\",\n          \"  Based on the provided context, it seems like the product being discussed is a lip mask that is available in various flavors and has received mixed reviews from customers. Some reviewers have reported that the product is not worth the hype and has caused dryness and irritation to their lips, while others have praised its hydrating and nourishing properties.\\n\\nWithout more information, it's difficult to provide a definitive answer to the question. However, I can offer some possible suggestions based on the context:\\n\\n1. If you have dry or chapped lips, you may want to consider using a lip balm or lip moisturizer instead of a lip mask. Look for products that contain ingredients like beeswax, coconut oil, or shea butter, which can help lock in moisture and protect your lips.\\n2. If you do decide to use a lip mask, be sure to follow the instructions carefully and avoid over-applying the product, as this can lead to dryness and irritation.\\n3. Consider using a lip mask that contains ingredients like hyaluronic acid, glycerin, or ceramides, which can help lock in moisture and support the natural barrier function of your lips.\\n4. If you experience any irritation or dryness after using a lip mask, try stopping use of the product and switching to a different brand or type of lip care product.\\n\\nI hope these suggestions are helpful! Let me know if you have any other questions.\",\n          \"  Based on the given context, it seems that the reviewer had mixed feelings about the product. They mentioned that they had purchased the product, but were disappointed with the results. They noted that the product did not live up to their expectations and that it left their lips feeling dry and chapped. However, they also mentioned that the product smelled good and had a nice texture.\\n\\nWithout more information, it is difficult to determine the specific negative effect that the reviewer experienced with the product. However, based on their comments, it seems that the product may not have been as effective as they had hoped in providing long-lasting hydration for their lips.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truth\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 52,\n        \"samples\": [\n          \"First night I used this I thought it was\\u00a0\",\n          \"Its not that this is bad but its also not good\\u00a0\",\n          \"Grapefruit is the best in the reviewer's opinion.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-eb155224-0932-4cf0-9d82-589ddfd2a443\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the primary use of the product</td>\n",
       "      <td>Based on the product reviews, the primary us...</td>\n",
       "      <td>[This product is amazing It really does smell ...</td>\n",
       "      <td>To ensure softlips by the time lipstick is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product is considered bearable by the</td>\n",
       "      <td>Based on the provided context, the product b...</td>\n",
       "      <td>[This product is amazing It really does smell ...</td>\n",
       "      <td>The peppermint scent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does the reviewer describe the product's</td>\n",
       "      <td>Based on the provided content, the reviewer ...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>Very hydrating and a little goes a long way.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the reviewer's opinion on the product</td>\n",
       "      <td>Based on the provided context, the reviewer'...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>The reviewer admits the product makes their lips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What negative effect did one reviewer</td>\n",
       "      <td>Based on the given context, it seems that th...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>It would leave a nasty film on their lips after</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>What is the reviewer's favorite scent for the</td>\n",
       "      <td>Based on the context, the reviewer's favorit...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>Grapefruit is the best in the reviewer's opinion.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>What did the reviewer learn after a few weeks</td>\n",
       "      <td>Based on the context, it seems that the revi...</td>\n",
       "      <td>[Received a sample and had to buy the full siz...</td>\n",
       "      <td>After a few weeks of use, the reviewer learned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Why did the reviewer give the product 2 stars?</td>\n",
       "      <td>Based on the provided context, it seems that...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>The reviewer gave it 2 stars for nice packaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>What was the reviewer's experience with the</td>\n",
       "      <td>Based on the provided context, I'll do my be...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>The reviewer did not have to reapply it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>What prompted the reviewer to purchase</td>\n",
       "      <td>Based on the context, the prompted reviewer ...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>After running through the sample size from a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 4 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb155224-0932-4cf0-9d82-589ddfd2a443')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-eb155224-0932-4cf0-9d82-589ddfd2a443 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-eb155224-0932-4cf0-9d82-589ddfd2a443');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-f2d43c2f-e8ec-4e10-85aa-a42d525b7200\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f2d43c2f-e8ec-4e10-85aa-a42d525b7200')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-f2d43c2f-e8ec-4e10-85aa-a42d525b7200 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                          question  \\\n",
       "0          What is the primary use of the product    \n",
       "1           product is considered bearable by the    \n",
       "2    How does the reviewer describe the product's    \n",
       "3   What is the reviewer's opinion on the product    \n",
       "4           What negative effect did one reviewer    \n",
       "..                                             ...   \n",
       "63  What is the reviewer's favorite scent for the    \n",
       "64  What did the reviewer learn after a few weeks    \n",
       "65  Why did the reviewer give the product 2 stars?   \n",
       "66    What was the reviewer's experience with the    \n",
       "67         What prompted the reviewer to purchase    \n",
       "\n",
       "                                               answer  \\\n",
       "0     Based on the product reviews, the primary us...   \n",
       "1     Based on the provided context, the product b...   \n",
       "2     Based on the provided content, the reviewer ...   \n",
       "3     Based on the provided context, the reviewer'...   \n",
       "4     Based on the given context, it seems that th...   \n",
       "..                                                ...   \n",
       "63    Based on the context, the reviewer's favorit...   \n",
       "64    Based on the context, it seems that the revi...   \n",
       "65    Based on the provided context, it seems that...   \n",
       "66    Based on the provided context, I'll do my be...   \n",
       "67    Based on the context, the prompted reviewer ...   \n",
       "\n",
       "                                             contexts  \\\n",
       "0   [This product is amazing It really does smell ...   \n",
       "1   [This product is amazing It really does smell ...   \n",
       "2   [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "3   [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "4   [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "..                                                ...   \n",
       "63  [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "64  [Received a sample and had to buy the full siz...   \n",
       "65  [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "66  [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "67  [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "\n",
       "                                         ground_truth  \n",
       "0         To ensure softlips by the time lipstick is   \n",
       "1                              The peppermint scent.   \n",
       "2       Very hydrating and a little goes a long way.   \n",
       "3   The reviewer admits the product makes their lips   \n",
       "4    It would leave a nasty film on their lips after   \n",
       "..                                                ...  \n",
       "63  Grapefruit is the best in the reviewer's opinion.  \n",
       "64    After a few weeks of use, the reviewer learned   \n",
       "65    The reviewer gave it 2 stars for nice packaging  \n",
       "66           The reviewer did not have to reapply it   \n",
       "67      After running through the sample size from a   \n",
       "\n",
       "[68 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "cc41990c91f3484aae7c7115e89a419e",
      "567f6c9eccf14bb1885280ffe56f9f3b",
      "57fd733182ec4b5e9370a37737b5a511",
      "f13caf8b02e747c59a3995ab5a8e8ff7",
      "64fac767e49140ce860327bd4bdfb0cd",
      "70a28e4195944f27a2f6f450352ff1cc",
      "39b2cb79577d4b5cbe03eadd7423104d",
      "c3793b132be741f0ba93a47a88f8c1e0",
      "e356868d7e8b4acaa7088f4b2c5b8c1c",
      "9c2b50dcaae143089dcf32b921011b38",
      "16cfb1e06c7c495f8debfba17069ee72"
     ]
    },
    "id": "gJ6v6y8PI2-o",
    "outputId": "0a877bb2-0557-4a48-8858-80f54099b852"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc41990c91f3484aae7c7115e89a419e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 147, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-16k in organization org-CKdljhinDy7lbNyNac0iDG3b on tokens per min (TPM): Limit 60000, Used 59623, Requested 4604. Please try again in 4.227s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 215, in _ascore\n",
      "    nli_result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-16k in organization org-CKdljhinDy7lbNyNac0iDG3b on tokens per min (TPM): Limit 60000, Used 59686, Requested 4477. Please try again in 4.163s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 147, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-16k in organization org-CKdljhinDy7lbNyNac0iDG3b on tokens per min (TPM): Limit 60000, Used 58631, Requested 4477. Please try again in 3.108s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 147, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-16k in organization org-CKdljhinDy7lbNyNac0iDG3b on tokens per min (TPM): Limit 60000, Used 59584, Requested 4489. Please try again in 4.073s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 147, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-16k in organization org-CKdljhinDy7lbNyNac0iDG3b on tokens per min (TPM): Limit 60000, Used 59553, Requested 4490. Please try again in 4.043s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 147, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-16k in organization org-CKdljhinDy7lbNyNac0iDG3b on tokens per min (TPM): Limit 60000, Used 56537, Requested 4460. Please try again in 996ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 147, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-16k in organization org-CKdljhinDy7lbNyNac0iDG3b on tokens per min (TPM): Limit 60000, Used 56831, Requested 4555. Please try again in 1.386s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 147, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-16k in organization org-CKdljhinDy7lbNyNac0iDG3b on tokens per min (TPM): Limit 60000, Used 59681, Requested 4704. Please try again in 4.385s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 147, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-16k in organization org-CKdljhinDy7lbNyNac0iDG3b on tokens per min (TPM): Limit 60000, Used 59597, Requested 4561. Please try again in 4.158s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 147, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-16k in organization org-CKdljhinDy7lbNyNac0iDG3b on tokens per min (TPM): Limit 60000, Used 58713, Requested 4585. Please try again in 3.298s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 147, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-16k in organization org-CKdljhinDy7lbNyNac0iDG3b on tokens per min (TPM): Limit 60000, Used 58970, Requested 4513. Please try again in 3.483s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 147, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-16k in organization org-CKdljhinDy7lbNyNac0iDG3b on tokens per min (TPM): Limit 60000, Used 58637, Requested 4579. Please try again in 3.216s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 147, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-16k in organization org-CKdljhinDy7lbNyNac0iDG3b on tokens per min (TPM): Limit 60000, Used 58845, Requested 4578. Please try again in 3.423s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 147, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-16k in organization org-CKdljhinDy7lbNyNac0iDG3b on tokens per min (TPM): Limit 60000, Used 59789, Requested 4612. Please try again in 4.401s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 147, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-16k in organization org-CKdljhinDy7lbNyNac0iDG3b on tokens per min (TPM): Limit 60000, Used 58016, Requested 4520. Please try again in 2.536s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 147, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Your account is not active, please check your billing details on our website.', 'type': 'billing_not_active', 'param': None, 'code': 'billing_not_active'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 147, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Your account is not active, please check your billing details on our website.', 'type': 'billing_not_active', 'param': None, 'code': 'billing_not_active'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_answer_correctness.py\", line 147, in _ascore\n",
      "    is_statement_present = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Your account is not active, please check your billing details on our website.', 'type': 'billing_not_active', 'param': None, 'code': 'billing_not_active'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_precision.py\", line 144, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Your account is not active, please check your billing details on our website.', 'type': 'billing_not_active', 'param': None, 'code': 'billing_not_active'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 147, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Your account is not active, please check your billing details on our website.', 'type': 'billing_not_active', 'param': None, 'code': 'billing_not_active'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_precision.py\", line 144, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Your account is not active, please check your billing details on our website.', 'type': 'billing_not_active', 'param': None, 'code': 'billing_not_active'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_precision.py\", line 144, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Your account is not active, please check your billing details on our website.', 'type': 'billing_not_active', 'param': None, 'code': 'billing_not_active'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_precision.py\", line 144, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Your account is not active, please check your billing details on our website.', 'type': 'billing_not_active', 'param': None, 'code': 'billing_not_active'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 204, in _ascore\n",
      "    answer_result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Your account is not active, please check your billing details on our website.', 'type': 'billing_not_active', 'param': None, 'code': 'billing_not_active'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_answer_relevance.py\", line 152, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 554, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 415, in generate\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 405, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 624, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1213, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 902, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 978, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1026, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 993, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Your account is not active, please check your billing details on our website.', 'type': 'billing_not_active', 'param': None, 'code': 'billing_not_active'}}\n",
      "ERROR:ragas.executor:Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 114, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 110, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 154, in _ascore\n",
      "    answers = await _output_parser.aparse(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/output_parser.py\", line 67, in aparse\n",
      "    output = await llm.generate(p_value)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 92, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 169, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 564, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 524, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 705, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 581, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1334, in create\n",
      "    return await self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1743, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1446, in request\n",
      "    return await self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1537, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 16413 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"result\",\n  \"rows\": 68,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"What is the primary benefit of the product\\u00a0\",\n          \"What is a user's opinion on the effectiveness of\\u00a0\",\n          \"How does the product generally make the lips\\u00a0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 68,\n        \"samples\": [\n          \"  Based on the provided context, the specific flavor of the product that the person is looking for is:\\n\\n\\\"Sweet Candy\\\"\\n\\nThe document mentions the following flavors:\\n\\n* Peppermint\\n* Grapefruit\\n* Gummy Bear\\n* Sweet Candy\\n\\nTherefore, the answer to the question \\\"What specific flavor of the product is...\\\" would be:\\n\\n\\\"Sweet Candy\\\"\",\n          \"  Based on the provided context, it seems like the product being discussed is a lip mask that is available in various flavors and has received mixed reviews from customers. Some reviewers have reported that the product is not worth the hype and has caused dryness and irritation to their lips, while others have praised its hydrating and nourishing properties.\\n\\nWithout more information, it's difficult to provide a definitive answer to the question. However, I can offer some possible suggestions based on the context:\\n\\n1. If you have dry or chapped lips, you may want to consider using a lip balm or lip moisturizer instead of a lip mask. Look for products that contain ingredients like beeswax, coconut oil, or shea butter, which can help lock in moisture and protect your lips.\\n2. If you do decide to use a lip mask, be sure to follow the instructions carefully and avoid over-applying the product, as this can lead to dryness and irritation.\\n3. Consider using a lip mask that contains ingredients like hyaluronic acid, glycerin, or ceramides, which can help lock in moisture and support the natural barrier function of your lips.\\n4. If you experience any irritation or dryness after using a lip mask, try stopping use of the product and switching to a different brand or type of lip care product.\\n\\nI hope these suggestions are helpful! Let me know if you have any other questions.\",\n          \"  Based on the given context, it seems that the reviewer had mixed feelings about the product. They mentioned that they had purchased the product, but were disappointed with the results. They noted that the product did not live up to their expectations and that it left their lips feeling dry and chapped. However, they also mentioned that the product smelled good and had a nice texture.\\n\\nWithout more information, it is difficult to determine the specific negative effect that the reviewer experienced with the product. However, based on their comments, it seems that the product may not have been as effective as they had hoped in providing long-lasting hydration for their lips.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truth\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 52,\n        \"samples\": [\n          \"First night I used this I thought it was\\u00a0\",\n          \"Its not that this is bad but its also not good\\u00a0\",\n          \"Grapefruit is the best in the reviewer's opinion.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_relevancy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3011024249424378,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 35,\n        \"samples\": [\n          0.7499999999625,\n          0.6388888888675925,\n          0.19355207607248953\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.34665591221319975,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 42,\n        \"samples\": [\n          0.20745616939787898,\n          0.8948498534744825,\n          0.19154119107146778\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_correctness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3095044271467096,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          0.0,\n          0.9052469021395904,\n          0.7728966410098237\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.33634782376792355,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 43,\n        \"samples\": [\n          0.8556416033798696,\n          0.19401462151319113,\n          0.6754801821477362\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.35258881447993395,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          0.7357543359059066,\n          0.7462188254012525,\n          0.8611407657659278\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"faithfulness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3250533519101111,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 44,\n        \"samples\": [\n          0.8048336407817414,\n          0.18655470635031313,\n          0.595915502725268\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-0abc9960-d314-4d1d-8dd0-2d4ee1c7ebe1\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>answer_similarity</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the primary use of the product</td>\n",
       "      <td>Based on the product reviews, the primary us...</td>\n",
       "      <td>[This product is amazing It really does smell ...</td>\n",
       "      <td>To ensure softlips by the time lipstick is</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product is considered bearable by the</td>\n",
       "      <td>Based on the provided context, the product b...</td>\n",
       "      <td>[This product is amazing It really does smell ...</td>\n",
       "      <td>The peppermint scent.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does the reviewer describe the product's</td>\n",
       "      <td>Based on the provided content, the reviewer ...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>Very hydrating and a little goes a long way.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the reviewer's opinion on the product</td>\n",
       "      <td>Based on the provided context, the reviewer'...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>The reviewer admits the product makes their lips</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What negative effect did one reviewer</td>\n",
       "      <td>Based on the given context, it seems that th...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>It would leave a nasty film on their lips after</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.940336</td>\n",
       "      <td>0.815743</td>\n",
       "      <td>0.203936</td>\n",
       "      <td>0.638889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>What is the reviewer's favorite scent for the</td>\n",
       "      <td>Based on the context, the reviewer's favorit...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>Grapefruit is the best in the reviewer's opinion.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.897188</td>\n",
       "      <td>0.768259</td>\n",
       "      <td>0.192065</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>What did the reviewer learn after a few weeks</td>\n",
       "      <td>Based on the context, it seems that the revi...</td>\n",
       "      <td>[Received a sample and had to buy the full siz...</td>\n",
       "      <td>After a few weeks of use, the reviewer learned</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.883797</td>\n",
       "      <td>0.839600</td>\n",
       "      <td>0.424186</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Why did the reviewer give the product 2 stars?</td>\n",
       "      <td>Based on the provided context, it seems that...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>The reviewer gave it 2 stars for nice packaging</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.845809</td>\n",
       "      <td>0.211452</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>What was the reviewer's experience with the</td>\n",
       "      <td>Based on the provided context, I'll do my be...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>The reviewer did not have to reapply it</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.841954</td>\n",
       "      <td>0.794771</td>\n",
       "      <td>0.912979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>What prompted the reviewer to purchase</td>\n",
       "      <td>Based on the context, the prompted reviewer ...</td>\n",
       "      <td>[Product Reviews  \\n \\nLip balm:  \\n \\nI bough...</td>\n",
       "      <td>After running through the sample size from a</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.966675</td>\n",
       "      <td>0.703564</td>\n",
       "      <td>0.390177</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 10 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0abc9960-d314-4d1d-8dd0-2d4ee1c7ebe1')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-0abc9960-d314-4d1d-8dd0-2d4ee1c7ebe1 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-0abc9960-d314-4d1d-8dd0-2d4ee1c7ebe1');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-b34a779f-b553-4d8f-be4b-6b6033a0b7d3\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b34a779f-b553-4d8f-be4b-6b6033a0b7d3')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-b34a779f-b553-4d8f-be4b-6b6033a0b7d3 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                          question  \\\n",
       "0          What is the primary use of the product    \n",
       "1           product is considered bearable by the    \n",
       "2    How does the reviewer describe the product's    \n",
       "3   What is the reviewer's opinion on the product    \n",
       "4           What negative effect did one reviewer    \n",
       "..                                             ...   \n",
       "63  What is the reviewer's favorite scent for the    \n",
       "64  What did the reviewer learn after a few weeks    \n",
       "65  Why did the reviewer give the product 2 stars?   \n",
       "66    What was the reviewer's experience with the    \n",
       "67         What prompted the reviewer to purchase    \n",
       "\n",
       "                                               answer  \\\n",
       "0     Based on the product reviews, the primary us...   \n",
       "1     Based on the provided context, the product b...   \n",
       "2     Based on the provided content, the reviewer ...   \n",
       "3     Based on the provided context, the reviewer'...   \n",
       "4     Based on the given context, it seems that th...   \n",
       "..                                                ...   \n",
       "63    Based on the context, the reviewer's favorit...   \n",
       "64    Based on the context, it seems that the revi...   \n",
       "65    Based on the provided context, it seems that...   \n",
       "66    Based on the provided context, I'll do my be...   \n",
       "67    Based on the context, the prompted reviewer ...   \n",
       "\n",
       "                                             contexts  \\\n",
       "0   [This product is amazing It really does smell ...   \n",
       "1   [This product is amazing It really does smell ...   \n",
       "2   [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "3   [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "4   [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "..                                                ...   \n",
       "63  [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "64  [Received a sample and had to buy the full siz...   \n",
       "65  [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "66  [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "67  [Product Reviews  \\n \\nLip balm:  \\n \\nI bough...   \n",
       "\n",
       "                                         ground_truth  answer_relevancy  \\\n",
       "0         To ensure softlips by the time lipstick is                NaN   \n",
       "1                              The peppermint scent.                NaN   \n",
       "2       Very hydrating and a little goes a long way.                NaN   \n",
       "3   The reviewer admits the product makes their lips                NaN   \n",
       "4    It would leave a nasty film on their lips after                NaN   \n",
       "..                                                ...               ...   \n",
       "63  Grapefruit is the best in the reviewer's opinion.               1.0   \n",
       "64    After a few weeks of use, the reviewer learned                1.0   \n",
       "65    The reviewer gave it 2 stars for nice packaging               0.0   \n",
       "66           The reviewer did not have to reapply it                1.0   \n",
       "67      After running through the sample size from a                1.0   \n",
       "\n",
       "    answer_similarity  answer_correctness  context_precision  context_recall  \\\n",
       "0                 NaN                 NaN                NaN             NaN   \n",
       "1                 NaN                 NaN                NaN             NaN   \n",
       "2                 NaN                 NaN                NaN             NaN   \n",
       "3                 NaN                 NaN                NaN             NaN   \n",
       "4                 NaN            0.940336           0.815743        0.203936   \n",
       "..                ...                 ...                ...             ...   \n",
       "63           1.000000            0.897188           0.768259        0.192065   \n",
       "64           1.000000            0.883797           0.839600        0.424186   \n",
       "65           1.000000            1.000000           0.845809        0.211452   \n",
       "66           0.841954            0.794771           0.912979        0.000000   \n",
       "67           0.966675            0.703564           0.390177        1.000000   \n",
       "\n",
       "    faithfulness  \n",
       "0            NaN  \n",
       "1            NaN  \n",
       "2            NaN  \n",
       "3            NaN  \n",
       "4       0.638889  \n",
       "..           ...  \n",
       "63      1.000000  \n",
       "64      1.000000  \n",
       "65      1.000000  \n",
       "66      1.000000  \n",
       "67      1.000000  \n",
       "\n",
       "[68 rows x 10 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    answer_similarity,\n",
    "    answer_correctness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    faithfulness,\n",
    "\n",
    ")\n",
    "\n",
    "result = evaluate(\n",
    "    #llm=llm_mistral_as,\n",
    "    dataset = dataset,\n",
    "    metrics=[\n",
    "        answer_relevancy,    # Scores the relevancy of the answer according to the given question.\n",
    "        answer_similarity,  # Scores the semantic similarity of ground truth with generated answer.\n",
    "        answer_correctness, # Measures answer correctness compared to ground truth as a combination of factuality and semantic similarity.\n",
    "        context_precision,  # Average Precision is a metric that evaluates whether all of the relevant items selected by the model are ranked higher or not.\n",
    "        context_recall,     # Estimates context recall by estimating TP and FN using annotated answer and retrieved context.\n",
    "        faithfulness,        # measures the factual consistency of the generated answer against the given context.\n",
    "\n",
    "\n",
    "    ], raise_exceptions=False\n",
    ")\n",
    "\n",
    "result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Y0AHNTdtJEa7"
   },
   "outputs": [],
   "source": [
    "df = result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "K6_StjnYJTv1",
    "outputId": "a207d578-d217-4c89-9c8a-a46500fcf3a0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"answer_relevancy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22.042338423138904,\n        \"min\": 0.0,\n        \"max\": 63.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          63.0,\n          0.7661468913585419,\n          0.8298302040977787\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22.0595255319453,\n        \"min\": 0.0,\n        \"max\": 63.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.700571462332745,\n          0.84195411554054,\n          63.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_correctness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22.390414639973283,\n        \"min\": 0.0,\n        \"max\": 64.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.7826975362033699,\n          0.8819754862071483,\n          64.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22.404593985220114,\n        \"min\": 0.0,\n        \"max\": 64.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.71036326299006,\n          0.8427043904202325,\n          64.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22.412773916610274,\n        \"min\": 0.0,\n        \"max\": 64.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.6990436107521552,\n          0.8232266896798872,\n          64.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"faithfulness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22.389100606118078,\n        \"min\": 0.0,\n        \"max\": 64.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.7671669134712703,\n          0.9118772667570259,\n          64.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-7874a705-f5b6-448f-afe3-af1fddfc60ed\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>answer_similarity</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>63.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.766147</td>\n",
       "      <td>0.700571</td>\n",
       "      <td>0.782698</td>\n",
       "      <td>0.710363</td>\n",
       "      <td>0.699044</td>\n",
       "      <td>0.767167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.301102</td>\n",
       "      <td>0.346656</td>\n",
       "      <td>0.309504</td>\n",
       "      <td>0.336348</td>\n",
       "      <td>0.352589</td>\n",
       "      <td>0.325053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.740925</td>\n",
       "      <td>0.408864</td>\n",
       "      <td>0.772830</td>\n",
       "      <td>0.591024</td>\n",
       "      <td>0.427183</td>\n",
       "      <td>0.768804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.829830</td>\n",
       "      <td>0.841954</td>\n",
       "      <td>0.881975</td>\n",
       "      <td>0.842704</td>\n",
       "      <td>0.823227</td>\n",
       "      <td>0.911877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981505</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7874a705-f5b6-448f-afe3-af1fddfc60ed')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-7874a705-f5b6-448f-afe3-af1fddfc60ed button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-7874a705-f5b6-448f-afe3-af1fddfc60ed');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-618f5433-fa11-4afb-bc68-d930cd042fd8\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-618f5433-fa11-4afb-bc68-d930cd042fd8')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-618f5433-fa11-4afb-bc68-d930cd042fd8 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "       answer_relevancy  answer_similarity  answer_correctness  \\\n",
       "count         63.000000          63.000000           64.000000   \n",
       "mean           0.766147           0.700571            0.782698   \n",
       "std            0.301102           0.346656            0.309504   \n",
       "min            0.000000           0.000000            0.000000   \n",
       "25%            0.740925           0.408864            0.772830   \n",
       "50%            0.829830           0.841954            0.881975   \n",
       "75%            1.000000           1.000000            1.000000   \n",
       "max            1.000000           1.000000            1.000000   \n",
       "\n",
       "       context_precision  context_recall  faithfulness  \n",
       "count          64.000000       64.000000     64.000000  \n",
       "mean            0.710363        0.699044      0.767167  \n",
       "std             0.336348        0.352589      0.325053  \n",
       "min             0.000000        0.000000      0.000000  \n",
       "25%             0.591024        0.427183      0.768804  \n",
       "50%             0.842704        0.823227      0.911877  \n",
       "75%             0.981505        1.000000      1.000000  \n",
       "max             1.000000        1.000000      1.000000  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C218x0y8JTyn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08cc0a022efd4abe9ced356efe614e19": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ac8fd9a81624e74afbff7e3f5c8b3c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11233b1143f940c7a512e1da1b72dffe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "13bcc8d456df484d864c568114478bd9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16cfb1e06c7c495f8debfba17069ee72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "27aa99cab69a4814b806756947fe1656": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "31d9968f13dd40759a3276b44649ae3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_747cb5eefa8d4dfdbce3f7375cb5e157",
      "placeholder": "​",
      "style": "IPY_MODEL_d72fec8d51544ed8a03a26778e444832",
      "value": "Evaluating: 100%"
     }
    },
    "39b2cb79577d4b5cbe03eadd7423104d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a7aa07d529e4b7dad80002b7ff814f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "563b4ec88b1344619816369cd217b8df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "567f6c9eccf14bb1885280ffe56f9f3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70a28e4195944f27a2f6f450352ff1cc",
      "placeholder": "​",
      "style": "IPY_MODEL_39b2cb79577d4b5cbe03eadd7423104d",
      "value": "Evaluating: 100%"
     }
    },
    "57fd733182ec4b5e9370a37737b5a511": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3793b132be741f0ba93a47a88f8c1e0",
      "max": 408,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e356868d7e8b4acaa7088f4b2c5b8c1c",
      "value": 408
     }
    },
    "64fac767e49140ce860327bd4bdfb0cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ef40e9c23884139a7a8dbd2d96d602c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13bcc8d456df484d864c568114478bd9",
      "placeholder": "​",
      "style": "IPY_MODEL_27aa99cab69a4814b806756947fe1656",
      "value": " 60/60 [03:07&lt;00:00, 12.58s/it]"
     }
    },
    "70a28e4195944f27a2f6f450352ff1cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71ab2aa9af3f46d49f9eaa9f34e4ad7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bb960b4ac0ef42ddac65f62fb656e7e7",
      "placeholder": "​",
      "style": "IPY_MODEL_11233b1143f940c7a512e1da1b72dffe",
      "value": " 40/40 [04:02&lt;00:00,  6.00s/it]"
     }
    },
    "747cb5eefa8d4dfdbce3f7375cb5e157": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83010bccbd8c4f0c916a318d3f211a96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_31d9968f13dd40759a3276b44649ae3d",
       "IPY_MODEL_ac4ddaeb47b3456d87529f80ce4ae266",
       "IPY_MODEL_6ef40e9c23884139a7a8dbd2d96d602c"
      ],
      "layout": "IPY_MODEL_08cc0a022efd4abe9ced356efe614e19"
     }
    },
    "8a322a3b98334ef99446e301c387aafc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91759ccd9a364ac5ae8206abf1870437": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "980f4bd382d14f8b9c62ac54f3a16068": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9c2b50dcaae143089dcf32b921011b38": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac4ddaeb47b3456d87529f80ce4ae266": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ac8fd9a81624e74afbff7e3f5c8b3c8",
      "max": 60,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_980f4bd382d14f8b9c62ac54f3a16068",
      "value": 60
     }
    },
    "ad8cc25ac5524edda74bdc207d56446d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8a322a3b98334ef99446e301c387aafc",
      "placeholder": "​",
      "style": "IPY_MODEL_3a7aa07d529e4b7dad80002b7ff814f2",
      "value": "Evaluating: 100%"
     }
    },
    "bb960b4ac0ef42ddac65f62fb656e7e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3793b132be741f0ba93a47a88f8c1e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca4d63c53a8d448e9a2f0ffde17afc08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cc41990c91f3484aae7c7115e89a419e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_567f6c9eccf14bb1885280ffe56f9f3b",
       "IPY_MODEL_57fd733182ec4b5e9370a37737b5a511",
       "IPY_MODEL_f13caf8b02e747c59a3995ab5a8e8ff7"
      ],
      "layout": "IPY_MODEL_64fac767e49140ce860327bd4bdfb0cd"
     }
    },
    "d72fec8d51544ed8a03a26778e444832": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e356868d7e8b4acaa7088f4b2c5b8c1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f13caf8b02e747c59a3995ab5a8e8ff7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c2b50dcaae143089dcf32b921011b38",
      "placeholder": "​",
      "style": "IPY_MODEL_16cfb1e06c7c495f8debfba17069ee72",
      "value": " 408/408 [28:03&lt;00:00, 23.39s/it]"
     }
    },
    "f22036d508534b598b63e3d197d14e56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ad8cc25ac5524edda74bdc207d56446d",
       "IPY_MODEL_f9833040f47a49f1ad8384a1182923f3",
       "IPY_MODEL_71ab2aa9af3f46d49f9eaa9f34e4ad7e"
      ],
      "layout": "IPY_MODEL_563b4ec88b1344619816369cd217b8df"
     }
    },
    "f9833040f47a49f1ad8384a1182923f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_91759ccd9a364ac5ae8206abf1870437",
      "max": 40,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ca4d63c53a8d448e9a2f0ffde17afc08",
      "value": 40
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
